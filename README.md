# Text Generation using Recurrent Neural Networks

This repository contains code for training a text generation model using recurrent neural networks (RNNs) implemented in PyTorch. The model is trained on a small corpus of text data and can generate new text based on a given input prompt.

## Requirements

- Python 3
- PyTorch
- NumPy

## Usage

1. Clone the repository:
``` https://github.com/yuvan-s-96/TNSDC-Generative-AI-Naan-Mudhalvan.git ```

2. Navigate to the project directory:
``` cd TNSDC-Generative-AI-Naan-Mudhalvan ```
3. Run the training script to train the model
```Language_Modelling.ipynb```

## Model Architecture

The text generation model architecture consists of an RNN layer followed by a fully connected layer. The RNN processes the input sequences and learns to predict the next character in the sequence.

## Data

The training data consists of a small corpus of text data, including phrases such as "hey how are you", "good i am fine", "have a nice day", and "where are you".

## References

- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [Karpathy, A. (2015). The Unreasonable Effectiveness of Recurrent Neural Networks.](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [Brownlee, J. (2019). Deep Learning for Natural Language Processing: Develop Deep Learning Models for Natural Language in Python.](https://machinelearningmastery.com/deep-learning-for-nlp/)



